{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d450aecc024ed1",
   "metadata": {},
   "source": [
    "# Estudio comparativo de PE usando características locales\n",
    "\n",
    "## Introdución\n",
    "En este notebook, se muestran las métricas obtenidas para los diferentes modelos propuestos. Mediante el dataset Household Power Consumption del portal UCI (https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption), estudiaremos la utilidad de cada uno de los mecanismos, y estableceremos cuál es el mecanismo a usar con otros datasets del estado del arte. El objetivo es estudiar el impacto de diferentes estrategias de encoding para ver cuál nos permite obtener mejores resultados en secuencias de larga extensión.\n",
    "\n",
    "Examinaremos las siguientes alternativas:\n",
    "\n",
    "| Codificación             | Descripción                                                       |\n",
    "|-------------------|------------------------------------------------------------------|\n",
    "| `no_pe`           | Sin codificación posicional, se usan solo los datos de entrada.      |\n",
    "| `informer`        | Codificación temporal original de Informer.|\n",
    "| `stats`           | Codificación basada en estadísticas por ventana temporal deslizante, que calcula media, std y valores extremos.        |\n",
    "| `stats_lags`      | Igual que `stats`, pero incluye lags como contexto local.    |\n",
    "| `all_pe_weighted` | Combinación de lo anterior, junto a PE fijos y PE aprendibles (LPE), ponderados con pesos normalizados mediante Softmax.   |\n",
    "| `tpe`             | Codificación temporal haciendo uso de temporal PE, t-PE, para aportar mayor información local. Contiene la información de lags, ventana y otros PE fijos, haciendo uso de pesos aprendidos y normalizados mediante Softmax. |\n",
    "| `spe`|Alteración del mecanismo de atención para incorporar información estadística sin necesidad de incrustarla en el embedding.|\n",
    "\n",
    "Tras dichas pruebas, examinaremos la mejor alternativa en base a las métricas y el costo computacional, y probaremos a experimentar si realmente la información añadida tiene impacto en la calidad de los resultados realizando Shuffle del contexto de entrada del encoder en test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84333949",
   "metadata": {},
   "source": [
    "## Positional encodings empleados\n",
    "\n",
    "Para comprender de manera fundamentada las alternativas propuestas anteriormente, podemos analizar nuestro punto de partida, Informer, y dos de las propuestas tomadas del paper de Habib Irani y Vangelis Metsis (https://arxiv.org/pdf/2502.12370): TPE y SPE.\n",
    "\n",
    "### Informer: modelo de partida\n",
    "\n",
    "Es el PE propuesto originalmente por Vaswani et al. (2017). Utiliza funciones sinusoidales para representar la posición:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "Donde cada variable indica:\n",
    "\n",
    "- **pos**: índice temporal (posición).\n",
    "- **i**: dimensión del componente del embedding.\n",
    "- **d**: dimensión total del embedding.\n",
    "\n",
    "Basándonos simplemente en su formulación, podemos distinguir ya sus ventajas y desventajas. Como ventaja, podemos ver que se trata de una codificación no entrenable, por lo que para unas mismas dimensiones de entrada de datos, el resultado será siempre el mismo, y sus valores no se deben entrenar época a época de manera independiente. Esto lo hace más sencillo de usar, y más eficiente al no requerir parámetros adicionales. Sin embargo, como principal desventaja, estimamos que no permite modelar con suficiente adaptabilidad los períodos no estacionales de datos o con mayor número de irregularidades y valores anómalos.\n",
    "\n",
    "\n",
    "### Codificación Posicional Aprendible: Learnable PE (LPE)\n",
    "\n",
    "En este otro enfoque, en lugar de fijar de antemano la codificación, como en la versión tradicional, establecemos que los parámetros son aprendibles por el modelo, y por tanto, requerirán ser ajustados durante las diferentes épocas, aumentando así la complejidad del modelo, pero permitiendo una mayor adaptabilidad a la forma de los datos de entrada.\n",
    "\n",
    "$$\n",
    "PE_{\\text{learnable}}(pos) = W_{pos}, \\quad W_{pos} \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "Se incorpora al embedding original de entrada de manera aditiva.\n",
    "\n",
    "$$\n",
    "X' = X + PE_{\\text{learnable}}[pos]\n",
    "$$\n",
    "\n",
    "Por tanto, las principales ventajas son la adaptabilidad al conjunto de datos de entrada, la posibilidad de que el propio modelo ajuste los pesos sin necesidad de realizarlo de manera fija e inflexible. Pero, como es notable en su definición, estamos aumentando el número de parámetros necesarios del modelo, por lo que supondrá un coste más elevado de entrenamiento, y además, a diferencia de los senos/cosenos, será necesario reentrenar la estructura de encoding si los datos de entrada tienen las mismas dimensiones pero distinto contenido, ya que la codificación ha de adaptarse a la forma de estos para minimizar el error.\n",
    "\n",
    "\n",
    "### Codificación Posicional Temporal (T-PE)\n",
    "\n",
    "Esta codificación está basada en el tradicional seno-coseno, que agrega la ya mencionada información global y geométrica a los datos, pero además, para mejorar la adaptabilidad a los datos, incorpora información semántica de estos. Para ello, utiliza una función de suavizado gaussiano para tratar de encontrar correspondecia entre cada par de posiciones i-j, de manera que se añada información de semejanza entre las posiciones.\n",
    "\n",
    "\n",
    "$$\n",
    "PE_{i, 2k} = \\sin\\left(\\frac{i}{10000^{\\frac{2k}{d}}}\\right), \\quad\n",
    "PE_{i, 2k+1} = \\cos\\left(\\frac{i}{10000^{\\frac{2k}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "Y para la información local:\n",
    "\n",
    "$$S(i, j) = \\exp\\left( -\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "Añadiéndose al encoding final de manera aditiva:\n",
    "\n",
    "$$T\\text{-}PE(i) = PE(i) + S(i, j)$$\n",
    "\n",
    "En resumen, conserva la sencillez de un encoding tradicional, pero con la determinación de similaridad entre posiciones.\n",
    "\n",
    "\n",
    "SPE (Stochastic Positional Encoding) introduce aleatoriedad controlada para mejorar la generalización y la captación de dependencias relativas.\n",
    "\n",
    "### Generación Estocástica Posicional \n",
    "\n",
    "SPE es un PE que, a diferencia de los anteriores, que bastaba con sumarlos con la información base del embedding, requieren modificar el mecanismo de atención para su correcto aprendizaje. Comienza con una matriz aleatoria con entradas gaussianas estándar i.i.d.:\n",
    "\n",
    "$$\n",
    "Z_d \\in \\mathbb{R}^{M \\times R}\n",
    "$$\n",
    "\n",
    "Luego, aplica convoluciones para generar las codificaciones:\n",
    "\n",
    "$$\n",
    "Q_d = Z_d \\bigotimes \\Phi_Q, \\quad K_d = Z_d \\bigotimes \\Phi_K\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- **$\\bigotimes$** representa la operación de convolución.\n",
    "- **( $\\Phi_Q, \\Phi_K$)** son filtros convolucionales aprendibles.\n",
    "\n",
    "La atención se calcula como:\n",
    "\n",
    "$$\n",
    "P_d \\approx \\frac{\\bar{Q}_d \\bar{K}_d^\\top}{R}\n",
    "$$\n",
    "\n",
    "Este producto aproximado retiene relaciones posicionales mediante convoluciones, y añade complejidad lineal que depende de la longitud de la secuencia.\n",
    "\n",
    "Sobre el papel, presenta varias ventajas:\n",
    "- Captura relaciones relativas sin requerir atención cuadrática.\n",
    "- Permite una mayor escalabilidad gracias a su menor costo computacional\n",
    "- Es más robusta ante sobreajuste por su componente estocástico, y podría ser eficaz en clasificación de series temporales largas y multivariadas.\n",
    "\n",
    "Sin embargo, aunque teóricamente pueda parecer más eficiente, modificar el mecanismo de atención puede ser crítico en función del conjunto de datos, ya que al no depender de estos, la codificación podría ser demasiado agnóstica del contexto, y ofrecer peores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9ee2b",
   "metadata": {},
   "source": [
    "Una vez explicadas los modelos, pasaremos a probarlo sobre el conjunto de datos HPC, tomando como baseline el modelo Informer sin ningún tipo de encoding, es decir, empleando únicamente para la construcción del embedding la información proporcionada directamente por los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a18aa327938b9",
   "metadata": {},
   "source": [
    "## Evaluación de los 7 modelos de PE mediante HPC\n",
    "\n",
    "Comenzaremos experimentando primero cuál de las 7 alternativas propuestas permite obtener un mejor resultados. En muchas de ellas, como ya se comentaba, se utilizan diversas estrategias de encoding ponderadas, ya que así el modelo puede decidir cuál de ellas es la que mejor se adapta al problema. Esto se debe a que, en ocasiones, la coexistencia de varios métodos no tiene por qué empeorar el resultado, ya que cada problema requiere unas necesidades específicas, en función de la correlación de los datos, y no existe una codificación de PE absoluta mejor en todos los casos.\n",
    "\n",
    "A continuación, procederemos a la lectura de los ficheros csv que contienen los resultados, y compararemos tanto en tabla como gráficamente sus resultados. Nos fijaremos especialmente en MAE Y MSE, recurriendo al resto de métricas cuando no exista una diferencia clara.\n",
    "\n",
    "Para tratar de no favorecer algoritmos concretos, todos serán evaluados siguiendo los parámetros aquí reflejados:\n",
    "\n",
    "- Ventana: 60\n",
    "- Longitud de secuencia: 180\n",
    "- Longitud de contexto: 60\n",
    "- Longitud de predicción: 60 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888b705b",
   "metadata": {},
   "source": [
    "### Comparativa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T17:03:54.841013Z",
     "start_time": "2025-06-29T17:03:54.812293Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ficheros con sus nombres de modelo\n",
    "model_files = {\n",
    "    \"All_PE_Weighted\": \"Experimentos/HPC_ALL_PE_v2.csv\",\n",
    "    \"No PE\": \"Experimentos/HPC_No_PE.csv\",\n",
    "    \"ALL_PE_Weighted (No Norm)\": \"Experimentos/HPC_ALL_PE_v2_NoNorm.csv\",\n",
    "    \"Informer\": \"Experimentos/metricas_InformerVanilla_hpc_informer_informer_HPCm_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_hpc_informer.csv\",\n",
    "    \"SPE\": \"Experimentos/HPC_SPE.csv\",\n",
    "    \"SPE + All_PE\": \"Experimentos/HPC_SPE_ALL.csv\",\n",
    "    \"Ventana (Stats)\": \"Experimentos/metricas_InformerVanilla_stats_informer_HPCm_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_stats.csv\",\n",
    "    \"TPE\": \"Experimentos/metricas_InformerVanilla_tpe_weighted_informer_HPCm_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_tpe_weighted.csv\",\n",
    "    \"Ventana (Stats + Lags)\": \"Experimentos/HPC_Ventana_Lags.csv\"\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for model_name, file_path in model_files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    # print(file_path)\n",
    "    df['Modelo'] = model_name  # Añadir columna del modelo\n",
    "\n",
    "    # Pivotar valores de Mean y STD\n",
    "    df_mean = df.pivot(index='Modelo', columns='Metric', values='Mean')\n",
    "    df_std = df.pivot(index='Modelo', columns='Metric', values='STD')\n",
    "\n",
    "    # Renombrar columnas para distinguir Mean y STD\n",
    "    df_mean.columns = [f\"{col}_Mean\" for col in df_mean.columns]\n",
    "    df_std.columns = [f\"{col}_STD\" for col in df_std.columns]\n",
    "\n",
    "    # Combinar ambos\n",
    "    df_combined = pd.concat([df_mean, df_std], axis=1)\n",
    "\n",
    "    dfs.append(df_combined)\n",
    "\n",
    "# Unir todos los modelos\n",
    "final_df = pd.concat(dfs)\n",
    "\n",
    "# Resetear índice y ordenar por MSE_Mean\n",
    "final_df.reset_index(inplace=True)\n",
    "final_df = final_df.sort_values(by=\"MSE_Mean\")\n",
    "\n",
    "# Mostrar\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce737de6aa4bef03",
   "metadata": {},
   "source": [
    "Podemos apreciar que el modelo con mejor rendimiento es el basado en varios encodings posicionales los cuales han sido normalizados y ponderados con pesos entre 0 y 1 mediante SoftMax. Son los que mejores resultados ofrecen en MSE, pero también está dentro del top cuando observamos MAE o RMSE, pero ahí si existen leves mejoras en otros modelos. Sin embargo, dado a la cercanía de estas otras alternativas, y a su mayor coste computacional, podríamos considerar que el modelo propuesto en la primera fila es aquel que debemos comparar y verificar con respecto al modelo original.\n",
    "\n",
    "Para ver los resultados de forma gráfica, podemos representar los valores de MSE y MAE en un gráfico de barras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_metricas_con_error(df,\n",
    "                            modelos_col='Modelo',\n",
    "                            metricas=['MAE_Mean', 'MSE_Mean'],\n",
    "                            stds=['MAE_STD', 'MSE_STD'],\n",
    "                            titulo='Comparación de métricas por modelo con desviación estándar'):\n",
    "    \"\"\"\n",
    "    Genera un gráfico de barras con barras de error (desviación estándar).\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene los datos\n",
    "    - modelos_col: nombre de la columna con los nombres de los modelos (default: 'Modelo')\n",
    "    - metricas: lista con los nombres de las columnas de métricas (default: ['MAE_Mean', 'MSE_Mean'])\n",
    "    - stds: lista con los nombres de las columnas de desviación estándar correspondientes (default: ['MAE_STD', 'MSE_STD'])\n",
    "    - titulo: string para el título del gráfico\n",
    "    \"\"\"\n",
    "    # Copiar solo las columnas necesarias\n",
    "    df_plot = df[[modelos_col] + metricas + stds].copy()\n",
    "\n",
    "    # Reemplazar NaNs en std por 0\n",
    "    for std_col in stds:\n",
    "        df_plot[std_col] = df_plot[std_col].fillna(0)\n",
    "\n",
    "    # Configuración de posiciones\n",
    "    x = df_plot[modelos_col]\n",
    "    x_pos = np.arange(len(x))\n",
    "    width = 0.35\n",
    "\n",
    "    # Crear figura\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Dibujar cada barra de métrica\n",
    "    for i, (metrica, std) in enumerate(zip(metricas, stds)):\n",
    "        offset = (-1)**i * width / 2  # alternar izquierda/derecha\n",
    "        ax.bar(x_pos + offset,\n",
    "               df_plot[metrica],\n",
    "               width=width,\n",
    "               yerr=df_plot[std],\n",
    "               capsize=5,\n",
    "               label=metrica.replace('_Mean', ''))\n",
    "\n",
    "    # Etiquetas\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(x, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Valor')\n",
    "    ax.set_title(titulo)\n",
    "    ax.legend()\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d510b433430a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T17:04:31.878483Z",
     "start_time": "2025-06-29T17:04:31.607193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mostramos métricas para HPC\n",
    "plot_metricas_con_error(final_df, titulo=\"Comparación de modelos para Household Power Consumption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7203e67185fd72",
   "metadata": {},
   "source": [
    "### Barajando los mejores modelos\n",
    "\n",
    "Una vez seleccionado el modelo que queremos comprobar, podemos pasar a ejecutar un experimento: comprobar si el positional encoding está realmente ayudando a aportar valor al modelo. Para ello, podemos realizar un pequeño experimento en el cual, mezclar los valores de entrada de referencia para el decoder, y ver si así se obtienen resultados similares a introducirlos originales.\n",
    "\n",
    "Lo que buscamos es ver si la información añadida por el PE realmente ayuda al modelo a aprender información local, o si por consiguiente, se está desaprovechando cómputo a cambio leves mejoras o incluso empeoramiento.\n",
    "\n",
    "Al igual que en el apartado anterior, procederemos a la lectura de resultados de 4 modelos:\n",
    "- Estadísticos + lags original\n",
    "- Estadísticos + lags desordenado\n",
    "- Informer original\n",
    "- Informer desordenado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c8ac22e527c75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T17:17:11.803472Z",
     "start_time": "2025-06-29T17:17:11.774102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ficheros con sus nombres de modelo\n",
    "model_files = {\n",
    "    \"ALL_PE\": \"Experimentos/HPC_ALL_PE_v2.csv\",\n",
    "    \"ALL_PE (Shuffled)\": \"Experimentos/HPC_ALL_PE_v2_Shuffled.csv\",\n",
    "    \"Informer\": \"Experimentos/metricas_InformerVanilla_hpc_informer_informer_HPCm_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_hpc_informer.csv\",\n",
    "    \"Informer (Shuffled)\": \"Experimentos/metricas_InformerVanilla_hpc_informer_shuffled_informer_HPCm_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_hpc_informer_shuffled.csv\",\n",
    "    \"No PE\": \"Experimentos/HPC_No_PE.csv\",\n",
    "    \"TPE\": \"Experimentos/metricas_InformerVanilla_tpe_weighted_informer_HPCm_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_tpe_weighted.csv\",\n",
    "    \"TPE (Shuffled)\": \"Experimentos/metricas_InformerVanilla_tpe_weighted_shuffled_informer_HPCm_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_tpe_weighted_shuffled.csv\",\n",
    "    \"PE (Fixed)\": \"Experimentos/metricas_InformerVanilla_hpc_pe_informer_HPCm_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebfixed_dtTrue_mxTrue_test_hpc_pe.csv\"\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for model_name, file_path in model_files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    #print(file_path)\n",
    "    df['Modelo'] = model_name  # Añadir columna del modelo\n",
    "\n",
    "    # Pivotar valores de Mean y STD\n",
    "    df_mean = df.pivot(index='Modelo', columns='Metric', values='Mean')\n",
    "    df_std = df.pivot(index='Modelo', columns='Metric', values='STD')\n",
    "\n",
    "    # Renombrar columnas para distinguir Mean y STD\n",
    "    df_mean.columns = [f\"{col}_Mean\" for col in df_mean.columns]\n",
    "    df_std.columns = [f\"{col}_STD\" for col in df_std.columns]\n",
    "\n",
    "    # Combinar ambos\n",
    "    df_combined = pd.concat([df_mean, df_std], axis=1)\n",
    "\n",
    "    dfs.append(df_combined)\n",
    "\n",
    "# Unir todos los modelos\n",
    "final_df = pd.concat(dfs)\n",
    "\n",
    "# Resetear índice y ordenar por MSE_Mean\n",
    "final_df.reset_index(inplace=True)\n",
    "final_df = final_df.sort_values(by=\"MSE_Mean\")\n",
    "\n",
    "# Mostramos tabla y gráfico\n",
    "display(final_df)\n",
    "plot_metricas_con_error(final_df, titulo=\"Comparación de modelos para Household Power Consumption (con barajado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ec70bd307065b",
   "metadata": {},
   "source": [
    "Podemos apreciar, claramente, que el impacto de mezclar la información de referencia en el encoder afecta de manera considerable a la alternativa que usa ventana de estadísticos, ya que se está rompiendo considerablemente la estructura temporal de los datos. Es un resultado razonable, pues mezclar la información dentro de las ventanas está alterando gravemente la información contenida localmente.\n",
    "\n",
    "Sin embargo, cuando realizamos el mismo procedimiento con el modelo Informer original, la diferencia es muy poco notable: pasamos apenas de 0.532 de MSE a 0.548. En el caso de las alternativas basadas en estadísticos, se pasa de 0.46 a 0.78, reflejando un empeoramiento significativo en los resultados a diferencia del modelo original.\n",
    "\n",
    "También se ha evaluado el modelo basado en TPE, debido a su menor varianza y cercanía a los estadísticos. En dicho caso, pasamos de 0.463 a 0.85, siendo aún más notable la diferencia.\n",
    "\n",
    "Por tanto, podríamos asumir que la información fija aportada por el PE sinusoidal de Informer no está añadiendo suficiente riqueza en datos de larga extensión como este dataset. Si bien, sí que mejora considerablemente el resultado contra un modelo sin ningún tipo de codificación (NoPe En el gráfico), la información añadida es puramente global, y en secuencias de esta longitud, podemos observar cambios de comportamiento a nivel local que pueden ser interesantes de tener en cuenta.\n",
    "\n",
    "A primera vista, el PE propuesto en ALL_PE_Weighted podría parecer superior, ya que estamos aprovechando dicha información local de utilidad. Sin embargo, tambiém debemos tener en cuenta el encoding basado en TPE, pues ofrece resultados también muy competitivos, y en otros datasets, podría ofrecer un buen resultado, ya que parece ser más estable en sucesivas ejecuciones (menor varianza), y tiene una inferencia más eficiente en tiempo. \n",
    "\n",
    "Por lo tanto, será también evaluado con el resto de datasets restantes, debido a que las diferencias en HPC han sido prácticamente marginales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69898f0",
   "metadata": {},
   "source": [
    "## Evaluación mediante otros datasets\n",
    "\n",
    "Una vez definidos los encodings a probar, `TPE` y `All_PE_Weighted`, procederemos a evaluar el rendimiento sobre otros 3 datasets conocidos en el estado del arte:\n",
    "- **ETTh1**, el dataset de funcionamiento de un sistema eléctrico referenciado en el paper original de Informer, y el cual fue usado para su análisis de rendimiento.\n",
    "- **Yellow Tripdata**, un conjunto de datos que recopila información acerca de viajes en taxi y su coste asociado. Resulta de interés debido a su posible estacionalidad en horas nocturnas y fines de semana, donde los precios son mayores, e identificar otros patrones (por ejemplo, hora de entrada y salida laboral, etc.). En adelante, para mayor comodidad, lo referenciaremos como `Taxi`.\n",
    "- **TINA**, cuyo acrónimo proviene de Time-series Industrial Anomaly, donde la gran tarea de interés reside en la detección anomalías y patrones en las mismas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ef796",
   "metadata": {},
   "source": [
    "### ETTh1\n",
    "\n",
    "Comenzaremos por el dataset de menor tamaño, ETT, el cual es ampliamente utilizado en el estado del arte para la comparativa entre nuevos modelos. Aunque su extensión es bastante reducida, nos permitirá ver el rendimiento del modelo que estamos proponiendo en otro tipo de series, y ver si es lo suficientemente elástico como para adaptarse a diferentes características.\n",
    "\n",
    "La configuración usada será la misma que en el paper original de Informer para establecer las mismas condiciones. En cuanto a la configuración de la ventana, cuando corresponde, y la longitud de la predicción, se ha establecido:\n",
    "\n",
    "- Ventana: 24\n",
    "- Longitud de secuencia: 96\n",
    "- Longitud de contexto: 48\n",
    "- Longitud de predicción: 24 (1 día)\n",
    "\n",
    "A continuación, importaremos los resultados y los graficaremos de la misma forma que el apartado anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39bb0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ficheros con sus nombres de modelo\n",
    "model_files = {\n",
    "    \"ALL_PE\": \"Experimentos/metricas_InformerVanilla_etth1_all_pe_informer_ETTh1_ftM_sl96_ll48_pl24_win24_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_etth1_all_pe.csv\",\n",
    "    \"ALL_PE (Shuffled)\": \"Experimentos/metricas_InformerVanilla_etth1_all_pe_shuffled_informer_ETTh1_ftM_sl96_ll48_pl24_win24_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_etth1_all_pe_shuffled.csv\",\n",
    "    \"Informer\": \"Experimentos/metricas_InformerVanilla_etth1_informer_informer_ETTh1_ftM_sl96_ll48_pl24_win24_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_etth1_informer.csv\",\n",
    "    \"Informer (Shuffled)\": \"Experimentos/metricas_InformerVanilla_etth1_informer_shuffled_informer_ETTh1_ftM_sl96_ll48_pl24_win24_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_etth1_informer_shuffled.csv\",\n",
    "    \"No PE\": \"Experimentos/metricas_InformerVanilla_etth1_nope_informer_ETTh1_ftM_sl96_ll48_pl24_win24_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_etth1_nope.csv\",\n",
    "    \"TPE\": \"Experimentos/metricas_InformerVanilla_etth1_tpe_informer_ETTh1_ftM_sl96_ll48_pl24_win24_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_etth1_tpe.csv\",\n",
    "    \"TPE (Shuffled)\": \"Experimentos/metricas_InformerVanilla_etth1_tpe_shuffled_informer_ETTh1_ftM_sl96_ll48_pl24_win24_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_etth1_tpe_shuffled.csv\"\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for model_name, file_path in model_files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    #print(file_path)\n",
    "    df['Modelo'] = model_name  # Añadir columna del modelo\n",
    "\n",
    "    # Pivotar valores de Mean y STD\n",
    "    df_mean = df.pivot(index='Modelo', columns='Metric', values='Mean')\n",
    "    df_std = df.pivot(index='Modelo', columns='Metric', values='STD')\n",
    "\n",
    "    # Renombrar columnas para distinguir Mean y STD\n",
    "    df_mean.columns = [f\"{col}_Mean\" for col in df_mean.columns]\n",
    "    df_std.columns = [f\"{col}_STD\" for col in df_std.columns]\n",
    "\n",
    "    # Combinar ambos\n",
    "    df_combined = pd.concat([df_mean, df_std], axis=1)\n",
    "\n",
    "    dfs.append(df_combined)\n",
    "\n",
    "# Unir todos los modelos\n",
    "final_df = pd.concat(dfs)\n",
    "\n",
    "# Resetear índice y ordenar por MSE_Mean\n",
    "final_df.reset_index(inplace=True)\n",
    "final_df = final_df.sort_values(by=\"MSE_Mean\")\n",
    "\n",
    "# Mostramos tabla y gráfico\n",
    "display(final_df)\n",
    "plot_metricas_con_error(final_df, titulo=f\"ETTh$_1$: comparación de encodings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49d149",
   "metadata": {},
   "source": [
    "Podemos apreciar resultados muy diferentes a los vistos para una secuencia más larga:\n",
    "\n",
    "- El rendimiento de All_PE siguen siendo el ganador, pero esta vez, con mayor diferencia, siendo esta vez de 0.04, que es 40 veces mayor a la diferencia existente en HPC. Además, la varianza de TPE es ahora mayor a la de ALL_PE, tal y como se puede verificar tanto numéricamente como en la barra de error.\n",
    "- El rendimiento de Informer, curiosamente, parece ser más cercano en este conjunto. Podemos ver que, además, cuando realizamos el mezclado, ahora es el modelo de Informer el que más impacto aprecia en sus resultados. De alguna manera, para secuencias más cortas, una simple codificación sinusoidal parece codificar adecuadamente la información.\n",
    "- En el caso de los dos modelos propuestos, pero mezclados, el empeoramiento sigue siendo notable como hasta ahora.\n",
    "\n",
    "Por tanto, este apartado nos arroja un resultado bastante interesante, y es que todo apunta que, a secuencias más largas, y de mayor complejidad, el PE tradicional basado en senos y cosenos tiene grandes carencias, potencialmente por la falta de aprovechamiento de la localidad, ya que series de gran tamaño podemos encontrar diferentes subsecuencias a las que debemos adaptarnos. Pero, cuando la serie es más corta, como esta, dicha información podría no ser tan variable, y de ahí que un enfoque global como Informer puedea seguir manteniendose cercano a los mecanismos locales.\n",
    "\n",
    "También podríamos sumar el impacto de la profundidad de las arquitecturas y estructuras de datos: a mayor tamaño de entrada, mayor número de parámetros, y por ende, más dificultad de propagación del aprendizaje del encoding a través de la arqutectura. Sin embargo, para verificarlo, debemos seguir evaluando más datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efddbf6",
   "metadata": {},
   "source": [
    "### Yellow Trip Data (Taxi)\n",
    "\n",
    "A continuación, es el turno de un dataset de extensión similar a HPC, por lo que se usarán los mismos parámetros que en este para la evaluación del rendimiento. De la misma forma, se han recopilado resultados para All_PE, TPE e Informer.\n",
    "\n",
    "- Ventana: 60\n",
    "- Longitud de secuencia: 180\n",
    "- Longitud de contexto: 60\n",
    "- Longitud de predicción: 60 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cea10d",
   "metadata": {},
   "source": [
    "Para facilitar la visualización de los datos, al ser valores muy pequeños, se ha creado una función adicional que cree el gráfico de barras en dos ejes diferentes, uno para MSE y otro para MAE, ya que los valore se encuentran en el orden de 10-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_metricas_por_separado(df,\n",
    "                               modelos_col='Modelo',\n",
    "                               metricas=['MAE_Mean', 'MSE_Mean'],\n",
    "                               stds=['MAE_STD', 'MSE_STD'],\n",
    "                               titulos=['MAE por modelo con desviación estándar',\n",
    "                                        'MSE por modelo con desviación estándar']):\n",
    "    \"\"\"\n",
    "    Genera gráficos de barras separados con barras de error (desviación estándar) para cada métrica.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene los datos\n",
    "    - modelos_col: nombre de la columna con los nombres de los modelos\n",
    "    - metricas: lista con los nombres de las columnas de métricas\n",
    "    - stds: lista con los nombres de las columnas de desviación estándar correspondientes\n",
    "    - titulos: lista de títulos para los gráficos\n",
    "    \"\"\"\n",
    "    df_plot = df[[modelos_col] + metricas + stds].copy()\n",
    "\n",
    "    # Reemplazar NaNs en std por 0\n",
    "    for std_col in stds:\n",
    "        df_plot[std_col] = df_plot[std_col].fillna(0)\n",
    "\n",
    "    x = df_plot[modelos_col]\n",
    "    x_pos = np.arange(len(x))\n",
    "    width = 0.5\n",
    "\n",
    "    # Crear una figura por métrica\n",
    "    for i, (metrica, std) in enumerate(zip(metricas, stds)):\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.bar(x_pos,\n",
    "               df_plot[metrica],\n",
    "               width=width,\n",
    "               yerr=df_plot[std],\n",
    "               capsize=5)\n",
    "\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(x, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Valor')\n",
    "        ax.set_title(titulos[i])\n",
    "        ax.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875193b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ficheros con sus nombres de modelo\n",
    "model_files = {\n",
    "    \"ALL_PE\": \"Experimentos/metricas_InformerVanilla_taxi_all_pe_weighted_informer_Taxi_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_taxi_all_pe_weighted.csv\",\n",
    "    \"PE (Fixed)\": \"Experimentos/metricas_InformerVanilla_taxi_pe_informer_Taxi_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebfixed_dtTrue_mxTrue_test_taxi_pe.csv\",\n",
    "    \"Informer\": \"Experimentos/metricas_InformerVanilla_taxi_informer_informer_Taxi_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_taxi_informer.csv\",\n",
    "    \"Informer (Shuffled)\": \"Experimentos/metricas_InformerVanilla_taxi_informer_shuffled_informer_Taxi_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_taxi_informer_shuffled.csv\",\n",
    "    \"No PE\": \"Experimentos/metricas_InformerVanilla_taxi_nope_informer_Taxi_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_taxi_nope.csv\",\n",
    "    \"TPE\": \"Experimentos/metricas_InformerVanilla_taxi_tpe_informer_Taxi_ftM_sl180_ll60_pl60_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_taxi_tpe.csv\",\n",
    "    #\"TPE (Shuffled)\": \"\"\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for model_name, file_path in model_files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    #print(file_path)\n",
    "    df['Modelo'] = model_name  # Añadir columna del modelo\n",
    "\n",
    "    # Pivotar valores de Mean y STD\n",
    "    df_mean = df.pivot(index='Modelo', columns='Metric', values='Mean')\n",
    "    df_std = df.pivot(index='Modelo', columns='Metric', values='STD')\n",
    "\n",
    "    # Renombrar columnas para distinguir Mean y STD\n",
    "    df_mean.columns = [f\"{col}_Mean\" for col in df_mean.columns]\n",
    "    df_std.columns = [f\"{col}_STD\" for col in df_std.columns]\n",
    "\n",
    "    # Combinar ambos\n",
    "    df_combined = pd.concat([df_mean, df_std], axis=1)\n",
    "\n",
    "    dfs.append(df_combined)\n",
    "\n",
    "# Unir todos los modelos\n",
    "final_df = pd.concat(dfs)\n",
    "\n",
    "# Resetear índice y ordenar por MSE_Mean\n",
    "final_df.reset_index(inplace=True)\n",
    "final_df = final_df.sort_values(by=\"MSE_Mean\")\n",
    "\n",
    "# Mostramos tabla y gráfico\n",
    "display(final_df)\n",
    "plot_metricas_por_separado(final_df, titulos=['Yellow Trip Data: MAE por PE',\n",
    "                                        'Yellow Trip Data: MSE por PE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dda8b1e",
   "metadata": {},
   "source": [
    "A la vista de los resultados, podemos extraer las siguientes conclusiones:\n",
    "\n",
    "- Los resultados pueden encontrarse en una escala bastante pequeña. Esto puede deberse, o bien, a que la amplitud de los valores de la serie son bastante pequeños, o bien, que se trata de una serie fácilmente aprendible, ya sea por mantener una tendencia clara o bien una estacionalidad sencilla de modelar. Tratándose de una recopilación de datos sobre carreras de Taxi, esto segundo podría ser la principal causa, ya que los precios son mayores de noche, y además, incrementan también los fines de semana, y en media, los viajes realizados serán bastante similares. Sin embargo, se requiere el estudio de estos datos en mayor profundidad mediante EDA para asegurar.\n",
    "\n",
    "- La mejor codificación, tanto para MAE como MSE, es la basada en TPE. Anteriormente, vimos que ofrecía valores similares a usar ALL_PE, pero en este caso, posiblemente su menor variabilidad, así como su incorporación a la tradicional fórmula sinusoidal de una componente exponencial dependiente de la información, permite tomar una mayor información local para el modelado. All_PE, sin embargo, sigue siendo una sólida alternativa, con un resultado bastante similar, aunque con prácticamente el doble de error de TPE.\n",
    "- A diferencia de los datasets anteriores, el modelo original de Informer no se distancia tanto en cuanto a resultados, y ocupa la tercera posición. Si procedemos además a evaluar lo que ocurre cuando se procede a desordenar la entrada del decoder, podemos apreciar que el error se duplica, demostrando que sí es capaz de recoger parte de la infomación temporal de los datos. \n",
    "- Por último, no usar ningún tipo de codificación no aporta prácticamente ninguna ventaja, y se trata del peor caso de todos. Es además el que mayor varianza recoge en sus resultados, ya que como no se agrega información adicional a los datos, el resultado es muy dependiente de inicialización.\n",
    "\n",
    "\n",
    "Por lo tanto, en este caso, podemos afirmar de manera evidente el correcto funcionamiento de TPE con respecto al resto de alternativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc28efc",
   "metadata": {},
   "source": [
    "### Dataset TINA\n",
    "\n",
    "Hasta ahora, se han ejecutado datasets de larga extensión temporal, como es el caso del anterior de Yellow Trip, o el asociado al consumo eléctrico HPC. Sin embargo, en cuanto número de variables, estos conjuntos de datos no tenían más de 7 variables. Pero, para probar la afectividad de los métodos, es necesario probar con datasets de complejidad en cuanto a variables. En este caso, TINA, cuyo conjunto de datos recopila mediciones de diferentes sensores de una acerería de ArcelorMittal (https://dasci.es/opendata/tina-time-series-industrial-anomaly-dataset/) es un buen candidato para medir la eficiencia de las codificaciones evaluadas cuando la complejidad reside sobre todo en el número de variables medidas por instante.\n",
    "\n",
    "Quedándonos únicamente con las variables numéricas, tenemos en total 103 características a estimar mediante el modelo, por lo que dado el cambio de tipología de problema, podemos apreciar resultados bastante diferentes a los anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ficheros con sus nombres de modelo\n",
    "model_files = {\n",
    "    \"ALL_PE\": \"Experimentos/metricas_InformerVanilla_tina_all_pe_opt_informer_TINA_ftM_sl288_ll96_pl48_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_tina_all_pe_opt.csv\",\n",
    "    \"Informer\": \"Experimentos/metricas_InformerVanilla_tina_informer_opt_informer_TINA_ftM_sl288_ll96_pl48_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_tina_informer_opt.csv\",\n",
    "    \"Informer (shuffled)\":\"Experimentos/metricas_InformerVanilla_tina_informer_opt_shuffled_informer_TINA_ftM_sl288_ll96_pl48_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_tina_informer_opt_shuffled.csv\",\n",
    "    \"No PE\": \"Experimentos/metricas_InformerVanilla_tina_not_pe_informer_TINA_ftM_sl288_ll96_pl48_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_tina_not_pe.csv\",\n",
    "    \"TPE\": \"Experimentos/metricas_InformerVanilla_tina_tpe_opt_informer_TINA_ftM_sl288_ll96_pl48_win60_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_test_tina_tpe_opt.csv\",\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for model_name, file_path in model_files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    #print(file_path)\n",
    "    df['Modelo'] = model_name  # Añadir columna del modelo\n",
    "\n",
    "    # Pivotar valores de Mean y STD\n",
    "    df_mean = df.pivot(index='Modelo', columns='Metric', values='Mean')\n",
    "    df_std = df.pivot(index='Modelo', columns='Metric', values='STD')\n",
    "\n",
    "    # Renombrar columnas para distinguir Mean y STD\n",
    "    df_mean.columns = [f\"{col}_Mean\" for col in df_mean.columns]\n",
    "    df_std.columns = [f\"{col}_STD\" for col in df_std.columns]\n",
    "\n",
    "    # Combinar ambos\n",
    "    df_combined = pd.concat([df_mean, df_std], axis=1)\n",
    "\n",
    "    dfs.append(df_combined)\n",
    "\n",
    "# Unir todos los modelos\n",
    "final_df = pd.concat(dfs)\n",
    "\n",
    "# Resetear índice y ordenar por MSE_Mean\n",
    "final_df.reset_index(inplace=True)\n",
    "final_df = final_df.sort_values(by=\"MSE_Mean\")\n",
    "\n",
    "# Mostramos tabla y gráfico\n",
    "display(final_df)\n",
    "plot_metricas_con_error(final_df, titulo=f\"Dataset TINA: Comparación de encodings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f0313",
   "metadata": {},
   "source": [
    "Como puede observarse, en este caso Informer logra un rendimiento notablemente superior en comparación con el resto de métodos evaluados:\n",
    "\n",
    "- El modelo TPE presenta un desempeño relativamente cercano, con una diferencia media en el error cuadrático medio (MSE) de apenas $10^{-2}$. No obstante, esta pequeña variación podría considerarse suficiente para afirmar que la versión original de Informer demuestra ventaja.\n",
    "- Por otro lado, el modelo All_PE obtiene resultados aún más próximos a los de TPE. En este caso, logra una menor varianza, aunque la diferencia podría deberse a variabilidad estadística más que a una ventaja estructural clara del encoding.\n",
    "\n",
    "- Tal como era esperable, la opción de no emplear ningún tipo de codificación posicional resulta ser la menos efectiva. Pero sí que es importante destacar que en este caso no se distancia tanto de las soluciones que sí incorporan información posicional, debido a que la complejidad de la serie dificulta su predicción a largo plazo y las diferencias se acortan.\n",
    "\n",
    "Sin duda, el dato más llamativo que podemos encontrar son los resultados del modelo original de Informer, pero con mezclado en su entrada: ofrece un mejor rendimiento que el resto de modelos. Se trata de un resultado poco intuitivo, ya que romper a propósito la temporalidad de los datos parece ir en contra de lo razonable. Buscar una respuesta a este comportamiento es bastante complicado; tal vez, podría ser furto del azar, ya que el mezclado es un proceso totalmente estocástico. Pero, al ser ejecutado en varias ocasiones para hacer el promedio, esta hipótesis puede prácticamente descartarse, ya que a mayor número de ejecuciones, más nos acercamos al rendimiento real del modelo. Posiblemente, este fenómeno sea atribuible a las propiedades del conjunto de datos en sí:\n",
    "\n",
    "- El conjunto de datos recoge gran cantidad de valores de diferentes sensores y localizaciones, las cuales han sido anonimizadas, por lo que es posible que algunas de estas propiedades fueran dependientes directamente entre sí, y provoquen redundancia que dificulte a los modelos codificados a extraer información útil.\n",
    "- Que las variables no tengan un comportamiento modelable con las características dadas. Puede que dependa de factores externos ajenos a los datos recogidos, los cuales no puedan ser directamente deducibles de los datos dados, y por tanto, dificulten el modelado correcto de la serie. O bien, que directamente su comportamiento no sea modelable al seguir algún tipo de distribución aleatoria o muy ruidosa: en ese caso, mezclar podría no afectar al resultado con tanto impacto como en los conjuntos anteriores debido a que la información temporal no aporta información útil sobre el comportamiento de cada variable.\n",
    "\n",
    "\n",
    "**Sin lugar a dudas, este caso requiere especial atención en la próxima reunión.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7742b6",
   "metadata": {},
   "source": [
    "### Dataset Public Load Power\n",
    "\n",
    "Este dataset es, sin duda, el de mayor longitud de todos los examinados en este resumen. Sin embargo, tal es su longitud que, para su ejecución, ha requerido asignar 100GB de RAM para evitar errores de OOM por parte del gestor de colas. Además, su tiempo de ejecución ya asciende a más de 14 días, pero aún se encuentra evaluando su primera época para el modelo con TPE, que es el que podría adaptarse mejor a este conjunto. Por lo tanto, ante su lentitud de ejecución, y la ocupación de los servidores de mayor potencia, ocupados al completo desde hace semanas, no se ha podido probar con otros algoritmos como Informer original. \n",
    "\n",
    "**La lentitud observada, sin embargo, podría llevarnos a reconsiderar si este dataset es adecuado en bruto o deberíamos de aplicar alguna técnica de reducción para obtener resultados en un tiempo razonable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e96c2c",
   "metadata": {},
   "source": [
    "## Resumen final\n",
    "\n",
    "Para resumir las claves de manera muy breve, se condensan aquí las principales conclusiones que, a mi juicio, se pueden extraer de los conjuntos de datos evaluados acerca de las codificaciones:\n",
    "\n",
    "- Cuanto mayor es la extensión de la serie, menor impacto posee la codificación empleada en los resultados. Se reducen las distancia entre los modelos con y sin codificación, e Informer comienza a no alejarse tanto de los modelos diseñados.\n",
    "- En general, los dos modelos escogidos, TPE y ALL_PE (que hace uso de pesos de ponderación para cada tipo de encoding) son los que ofrecen los mejores resultados en la mayoría de casos, siendo TPE más estable numéricamente en sucesivas ejecuciones.\n",
    "- Los conjuntos de mayor extensión, como TINA y PLP, ofrecen comportamiento bastante anómalos frente a los anteriores, obteniendo TINA un mejor rendimiento cuando se mezcla su entrada de contexto al decoder, y PLP imposible de ejecutar con los recursos actuales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Informer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
