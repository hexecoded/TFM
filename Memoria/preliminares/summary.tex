% !TeX root = ../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Summary
%*******************************************************

\selectlanguage{english}
\chapter{Summary}

In a data-driven world, where we generate overwhelming volumes of information every day, we are faced with highly diverse structures that determine the tools available for processing and extracting knowledge. Recent Transformer-based models~\cite{vaswani2023attentionneed} have made possible to handle such data more effectively, achieving milestones that were far from being possible just a few years ago. These models have a remarkable capacity to capture complex dependencies and to adapt across diverse domains, ranging from their original purpose, natural language processing, to very different ones such as computer vision.\\
Among all these structures, one specific domain stands out: time series data. Time series record information about phenomena sampled over time, which may contain one or more attributes that characterize them, and graphically allow us to study temporal behavior in a very intuitive way. Until other models were made, classical statistical techniques have been employed for this purpose. However, with the rise and versatility of Transformers, efforts have been made to adapt this tool for use as a classifier or predictor of future series behavior.\vspace{0.35em}

Long-term prediction, commonly referred to as \textit{forecasting}, remains as one of the most active areas of research. The challenge arises from the fact that long-horizon predictions have typically relied on iteratively feeding shorter-horizon forecasts back into the model, leading to cumulative error propagation. As a result, the outcomes differ substantially from what was expected.

Transformers appeared to mitigate this limitation, as their generative nature enables them to learn sequential structures and model input content more effectively. However, despite their similarities with natural language, time series differ in essential ways: they consist of numerical values that are strictly tied to their temporal stamps, and any disruption in this order contradicts the very nature of the data. 
To address sequence order, Transformers incorporate additional information with positional encodings, combining each embedding with additional components that reflect its position within the sequence. This mechanism, originally based on trigonometric functions, has proven effective in text and was directly inherited in the context of time series. However, empirical evidence suggests that such encodings are not well suited to this domain~\cite{zeng2022transformerseffectivetimeseries}.\vspace{0.35em}

This is due to the lack of semantics in the encoding itself, which considers only absolute positions without accounting for local context. In time series, local information and the detection of recurring patterns are critical, making necessary to create a new encoding method capable of modeling this information with time series as the main target.\vspace{0.35em}

This work takes a step in that direction and proposes a set of encoding methods based on a novel concept: augmenting positional information with contextual features, so that each timestamp carries information about its local environment, including average behavior, variance, and differences with preceding values. This design ensures that two positions of similar properties obtain similar encodings, enabling the model to align them effectively. As a result, model performance improves in seasonal scenarios, where the series repeats its behavior periodically.\vspace{0.35em}

Furthermore, we introduce the notion of combining multiple positional encodings from the state of the art within a single embedding, integrated through a weighted summation. The weights are learned by the model, allowing the encodings to complement each other while minimizing the need for manual parameter specification.\\
Both proposals have been evaluated using open-access datasets that collect information about different domains, such as electricity transformer systems, industrial machinery sensors, and public transport usage records. The objective is to examine long-term forecasting performance in multiple contexts, with the goal of advancing predictive quality to enable optimized system operations.\vspace{0.35em}

All models have been evaluated in terms of both computational efficiency and predictive performance. This analysis enabled us to identify the approach with the strongest generalization capacity across the full set of experiments. The results demonstrate substantial improvements, with average gains ranging from 10\% to 30\%, and in some high-precision cases reaching up to a 90\% improvement compared to a widely recognized state-of-the-art baseline, Informer~\cite{zhou2021informerefficienttransformerlong}.
% Al finalizar el resumen en inglés, volvemos a seleccionar el idioma español para el documento
\selectlanguage{spanish} 
\endinput
