% !TeX root = ../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Summary
%*******************************************************

\chapter{Resumen}

En un mundo basado en los datos, donde diariamente generamos volúmenes inmanejables de información, nos enfrentamos a estructuras y semánticas muy diversas que condicionan las herramientas con el que podemos procesarlos y extraer información. Modelos recientes basados en Transformers~\cite{vaswani2023attentionneed} nos han permitido modelarlos y procesarlos de manera más sencilla, alcanzando objetivos prácticamente impensables hace unos años. Poseen una gran capacidad para capturar dependencias complejas y adaptarse a diferentes dominios, que abarcan desde su propósito original, el procesamiento de lenguaje natural, hasta otros muy diferentes, como las imágenes.\\
Entre todas estas estructuras, destaca un soporte concreto: las series temporales. Estas recogen información acerca de fenómenos muestreados a lo largo del tiempo, que pueden contener uno o más atributos que lo caracterizan, y que, de manera gráfica, nos permiten estudiar el comportamiento temporal de manera muy intuitiva. Hasta el momento, se han empleado diferentes técnicas clásicas basadas en la estadística, pero con el gran auge y versatilidad del Transformer, se ha tratado de adaptar esta herramienta para usarla como clasificador y predictor de futuros comportamientos de la serie.\vspace{0.35em}

La predicción a largo plazo, conocida en inglés como \textit{forecasting}, es uno de los principales frentes abiertos en la investigación. Esto se debe a que hasta el momento, las predicciones que requieren horizontes de larga duración se apoyaban en los valores predichos en un horizonte menor, lo cual fomentaba la acumulación errores de manera incremental hasta obtener resultados totalmente diferentes a lo que se esperaba. \vspace{0.35em}

Con esta nueva arquitectura, esta problemática parecía desaparacer, ya que estos pueden actuar de manera generativa con sus entradas, aprendiendo su estructura secuencial y siendo capaces de modelar su contenido. Sin embargo, a pesar de sus similaridades con el lenguaje natural, las series temporales difieren considerablemente de estas: disponemos de valores numéricos, que dependen de manera muy estricta de su instante temporal (timestamp), y cualquier ruptura en ese orden va en contra de la propia naturaleza de los datos.
Originalmente, los Transformers incorporan información adicional en sus secuencias de entrada para no perder el orden estructural de los datos. Para ello, enriquecen la entrada componiendo el embedding, construido por cada elemento de entrada más una serie de componentes que reflejan su posición dentro de la secuencia. Este mecanismo, llamado positional encoding, ha empleado hasta el momento mecanismos sencillos como la definición de posiciones mediante funciones trigonométricas, efectivas en texto, y que fueron heredadas en el nuevo dominio de las series temporales. Pero, en la práctica, se ha demostrado que no son un mecanismo efectivo ~\cite{zeng2022transformerseffectivetimeseries} para este ámbito.\vspace{0.35em}

Esto se debe a la falta de semántica del propio encoding, que no tiene en cuenta la información del entorno de cada instante temporal, más allá del orden absoluto establecido por la codificación original. En series temporales, la información local y la detección de patrones repetitivos es decisivo, y por tanto, es necesario crear un nuevo método de codificación capaz de modelar esta información con las series temporales como formato objetivo.\vspace{0.35em}

Este trabajo da un paso en ese aspecto, y propone una serie de métodos de codificación basados en un nuevo concepto novedoso: concatenar información adicional a las posiciones de entrada, de manera que cada instante medido llevo consigo información de contexto local que recoja comportamiento medio, desviación, y valores diferenciados con instantes anteriores, de forma que dos posiciones de igual naturaleza posean una codificación similar, y el modelo sea capaz de hacerlas corresponder. De esta forma, conseguimos mejorar el rendimiento de modelos en contextos estacionales, donde la serie repita su comportamiento periódicamente.\vspace{0.35em}

Adicionalmente, incorpora la idea de hacer coexistir a varias codificaciones, existentes en el estado del arte, en un mismo embedding, fruto sumatorio ponderado, de forma que estas se complementen entre sí, pero empleando pesos de ponderación aprendidos por el propio modelo, que reducen al mínimo la necesidad de especificación de parámetros de entrada.\\
Ambas propuestas han sido evaluadas empleando datasets de acceso abierto que recogen información sobre diferentes fenómenos, como sistemas transformadores de electricidad, sensores de maquinaria industrial y medidas de uso del transporte público, de manera que se predigan en horizontes a largo plazo en diferentes dominios, y se logre, como objetivo final, mejorar la predicción para optimizar la gestión de dichos sistemas.\vspace{0.35em}

Se han evaluado los resultados de todos los modelos, considerando tanto la eficiencia computacional como la mejora en las métricas de rendimiento. Con ello, ha sido posible identificar la propuesta que demuestra una mayor capacidad de generalización frente al conjunto completo de experimentos realizados. Los resultados obtenidos ofrecen márgenes de mejora razonables, de incluso entre un 10-90\%  en promedio, siendo en casos concretos de alta precisión una mejora de hasta el 90\% con respecto a los resultados de un modelo consagrado del estado del arte, como lo es Informer~\cite{zhou2021informerefficienttransformerlong}. 

