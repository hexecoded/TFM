\chapter{Conclusiones y trabajos futuros}

Tras cerrar la fase de experimentación, junto con las etapas previas de investigación, análisis e implementación de los modelos, podemos extraer numerosas conclusiones que nos permiten evaluar, de manera global, poner en valor los resultados obtenidos durante el desarrollo del proyecto, y explorar posibles líneas de investigación futuras en relación con este trabajo.\\

\section{Conclusiones}

Durante el estudio, hemos conseguido obtener conclusiones de gran relevancia. En primer lugar, hemos conseguido identificar las principales carencias presentes en los modelos de aprendizaje basado en Transformers en series temporales. Gracias a un análisis exhaustivo de los modelos existentes, y principalmente, de las codificaciones posicionales empleadas para su funcionamiento, hemos detectado que la principal problemática en estos es la falta de información local y temporal. Cuando se aplican codificaciones tradicionales basada en patrones geométricos, no se captura adecuadamente el orden de estos, pues únicamente se ha tenido en cuenta su orden absoluto, y no se emplea la información relativa de los diferentes instantes para complementarla.\\

Debido a ello, se han explorado diferentes variantes capaces de paliar este efecto, retornando como base el concepto de ventana deslizante, ya empleado en algoritmos clásicos como método de diferenciación y extracción local de información, pero usándolo en su lugar para la obtención local de información entorno a cada punto y variable medidos, de forma que se enriquece su contexto agregando nuevas características asociada con valores como la media, desviación y extremos del entorno, así como información de diferencial con otros puntos temporales cercanos. Dichos modelos están disponibles en el repositorio de GitHub\footnote{Repositorio: \url{https://github.com/hexecoded/tfm}}, donde puede encontrar un entorno de experimentación flexible, que permite evaluar diferentes conjuntos de entrada.\\

Para la evaluación de las variantes de positional encoding propuestas, la disponibilidad de datos abiertos y accesibles públicamente ha sido fundamental, ya que ha permitido trabajar con conjuntos provenientes de diferentes dominios, como sistemas eléctricos, datos industriales o de transporte público. Su diferente estructura y metodología de medición han requerido realizar algunas adaptaciones, dado que la procedencia de los datos de diferentes países planteaba desafíos como el correcto formateo de las fechas, aspecto esencial en el análisis de series temporales. Sin embargo, esto no ha impedido la obtención un conjunto de datos adecuado y suficiente para lograr un entorno de evaluación robusto y fiable.\\

El desarrollo de la metodología basada en la información por ventanas, y la ponderación aditiva de modelos de codificación de diferente naturaleza han sido claves, ya que, a excepción del tamaño de dicha ventana, facilitamos el proceso de selección al analista de datos al no necesitar de especificación de parámetros adicionales para el equilibrado de componentes, y además, se consiguen mejoras significativas en el rendimiento del modelo. El uso de mecanismos de normalización para estos elementos permite controlarlos sin problemas junto al resto de componentes, a excepción del tamaño del ventana, que requiere ser establecido de antemano. Sin embargo, dicho valor ha sido acotado heurísticamente, por lo que el proceso de decisión se ve considerablemente reducido.
Disponiendo de este concepto como base, no sólo se han propuesto nuevas alternativas fácilmente interpretables, sino que también se ha permitido combinar esta mejora con los modelos de encoding ya existentes, sin necesidad de escoger selectivamente entre candidatos concretos, sino que podemos combinar de forma aditiva tantas componentes como se deseen.\\

Por último, gracias al entorno construido mediante Python y Pytorch, hemos conseguido evaluar de manera rigurosa cada una de las propuestas realizadas, así como encontrado aquellas que ofrecían el mejor rendimiento, donde destaca nuestra alternativa llamada WinStatFlex. Sin embargo, una de las principales desventajas de esta propuesta es la eficiencia computacional, ya que aumenta considerablemente con respecto a los modelos tradicionales. Pero este sobrecoste puede verse compensado por las métricas obtenidas, ya que la aparente eficiencia de los modelos tradicionales se debe principalmente a la simplicidad de su embedding y la limitada capacidad de la codificación posicional. El mayor coste se asocia a la incorporación de información adicional, previamente considerada implícita, pero que en realidad actuaba de manera limitada, como se mostró previamente en \cite{zeng2022transformerseffectivetimeseries}.

\section{Trabajos futuros}

Todos estos aspectos dejan múltiples preguntas abiertas, las cuales abren la puerta a posibles trabajos futuros centrados en el desarrollo de nuevas metodologías y en la integración de esta forma de codificación con las ya existentes, pues el campo de investigación en series temporales es muy amplio y aún poco explorado. En primer lugar, podemos destacar la extensión de esta metodología a otros ámbitos, como la clasificación de series temporales y la detección de anomalías. Estas tareas también pueden verse beneficiadas de un enfoque más flexible, empleando información local, mejorando así los resultados de ambos problemas.\\ La clasificación de series temporales puede ser relevante desde el punto de vista médico, donde el diagnóstico puede beneficiarse más de un enfoque que infiere el tipo de patología que tiene el paciente más que de predecir su comportamiento. Esto es aplicable a las enfermedades cardíacas, o a las que involucran el comportamiento de las ondas cerebrales, como la epilepsia. En cuanto a la detección de anomalías, adaptar su uso puede ser especialmente interesante en entornos industriales, como ya hemos visto en TINA, donde se busca determinar cuando un determinado componente podría fallar, o si existe alguna perturbación que pudiese comprometer la calidad de la producción.\\

En lo que respecta a los conjuntos de datos, la limitación de recursos de cómputo no nos ha permitido realizar un análisis más amplio sobre una mayor variedad de semánticas. Por tanto, el empleo de otros conjuntos de datos, propios de ámbitos como la medicina, física o la meteorología, puede ayudarnos a encontrar nuevos patrones comunes con los que acotar el funcionamiento de nuestro encoding aditivo, y reducir el número de componentes empleadas para mejorar el rendimiento.\\

Fusionar las mecánicas aquí encontradas con estrategias ya existente también es una tarea interesante. Existen modelos, como el ya analizado FEDformer \cite{zhou2022fedformerfrequencyenhanceddecomposed}, que emplean transformadas de Fourier y Wavelet para obtener información más allá del dominio temporal, por lo que realizar dicho aprendizaje en estos dominios podría suponer una ventana considerable, dotando al modelo de mayor visión del problema.\\

Por último, de cara a mejorar la gestión de los recursos, y aprovechar al máximo el hardware disponible, un enfoque distribuido de este modelo podría permitirnos reducir de manera considerable el coste asociado a la mayor complejidad de codificación del embedding, consiguiendo reducir la brecha existente con los modelos tradicionales más eficientes. Además, esto también abre la puerta al aprendizaje federado, con el objetivo de realizar predicciones y detección de anomalías en diferentes localizaciones sin necesidad de hacer navegar por la red los datos analizados. Esto sería de especial interés en entornos competitivos como el mundo empresarial, o en ámbitos médicos donde realizar intercambio de datos puede ser especialmente delicado por las leyes de protección de datos.\\

A nivel personal, este proyecto me ha permitido reforzar mis conocimientos acerca de los grandes modelos de actualidad, como son los Transformers, y apreciar de cerca su gran potencial para tareas más allá del procesamiento de lenguaje natural. Me ha permitido poner en valor que, aunque se trata de un modelo extremadamente versátil, su adaptación a nuevos ámbitos, como ha sido en este caso las series temporales, requieren una especial atención al detalle y comprensión de cada una de sus componentes, pues, a pesar de que las series temporales también son datos estructurados de forma secuencial, la dependencias y el orden de estos puede perderse o degradarse de forma fácil si no se establecen los mecanismos oportunos.\\

La búsqueda de nuevos mecanismos que permitan mantener dicha cohesión estructural de los datos ha sido especialmente difícil. La complejidad de encontrar un mecanismo que realmente propusiese una mejora a los modelos existentes, o al menos, permitiera abrir una línea nueva frente a estos es alta, pues en numerosas ocasiones, las codificaciones eran mucho peores que incluso no emplear codificación, haciendo que gran parte de la realización de este proyecto fuese ensayo y error. Sin embargo, todo el proceso me ha permitido ver de cerca el proceso habitual investigador, y la importancia de persistir y reorganizar las ideas para poder lograr el objetivo. Trabajar con grandes conjuntos de datos y modelos que requieren millones de parámetros me ha permitido poner aún más en estima la importancia de lograr modelos eficientes y claros, que sean capaces de procesar los datos sin operaciones innecesarias, y que mantengan su explicabilidad, ya que toda operación superflua puede inducir grandes diferencias en el tiempo de ejecución final.