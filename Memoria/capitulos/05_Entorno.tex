\chapter{Modelo escogido y entorno de trabajo}
\label{cap5}

Una vez disponemos de los conjuntos de datos seleccionados, procesados, y establecidos los métodos de codificación que queremos probar, podemos pasar a la construcción del entorno de trabajo. De por sí solos, los métodos de encoding no puede ser evaluados, ya que requieren su aplicación en un modelo concreto que lo acepte de entrada y sea capaz de procesarlo y aprender del embedding proporcionado.\\

En la sección del estado del arte (Capítulo \ref{sota}), estudiamos multitud de modelos basados en Transformers que podríamos usar como base para nuestras pruebas. Sin embargo, en varios de los casos, los mecanismos de atención eran muy elaborados, y podrían dificultar apreciar el impacto del positional encoding evaluado. Por ello, para su evaluación, se ha optado por tomar una base similar a Informer, que nos permitirá encontrar un equilibrio entre un modelo adaptado a series temporales y un modelo lo suficientemente simple para ser comprendido y modificado. Se profundizará la justificación de esta elección en la Sección \ref{modelo}.\\

Durante el resto de este Capítulo, estudiaremos además las métricas seleccionadas para la comparación de las propuestas y la configuración del entorno de experimentación escogido.

\section{Modelo seleccionado: Informer}
\label{modelo}

El modelo seleccionado sobre el que evaluar el conjunto de datos es un elemento clave a la hora de medir el rendimiento de los mismos. En la actualidad, podemos encontrar una gran variedad de modelos, algunos más sofisticados y otros más sencillos, que aplican multitud de operaciones y tratan de extraer el mayor rendimiento posible del conjunto de datos. En este caso, deseamos un modelo que sea capaz de adaptarse correctamente a la estructura de una serie temporal, pero que además mantenga intacto el concepto de Transformer, que es el entorno de trabajo en el cual operamos mediante el enriquecimiento del encoding. Si bien podríamos operar con modelos que no utilizan atención, esta decisión podría dificultar medir el impacto real del encoding de los resultados, ya que en modelos ya vistos como Autoformer, la codificación está implícita dentro de la propia arquitectura mediante la autocorrelación, y resulta más complejo apreciar el impacto del encoding, o probar el caso extremo de suprimirlo completamente para obtener nuestro baseline.\\

Por estos motivos, se ha optado por hacer uso de un modelo muy conocido y comparado en el estado del arte como es \textit{Informer}. Recordando su arquitectura de definición, este modelo destacaba por el uso de ProbSparse attention para reducir el coste del cómputo de atención a $O(n \log{n})$, empleaba el método de \textit{distillation} para reducir el tamaño de entrada de cada capa del encoder/decoder, y usaba un decoder estilo generador que permite simplificar el proceso de inferencia y predicción. Sin embargo, existe un aspecto clave que podría entrar en conflicto con nuestro problema el uso de ProbSparse.\\

Tal y como estudiamos en la sección \ref{attinf} del estado del arte, \textit{ProbSparse} conseguía reducir el tamaño del producto realizado en atención realizando un muestreo mediante una distribución logarítmica de los tokens de entrada, con la teoría de escoger únicamente aquellos realmente representativos en el problema, y sustituyendo el resto por un valor medio predefinido que reduzca el número de operaciones.\\
	
Desde el punto de vista computacional, este proceso puede ser interesante y útil para reducir la complejidad del problema considerado, pero está afectando negativamente al desempeño de la codificación: en caso de usar encodings que favorezcan el contexto local, como nuestro caso mediante el uso de la ventana, o métodos semánticos como TPE, suprimir valores estimados como de ``escaso valor'' podría realmente estar eliminando puntos esenciales para estas codificaciones, y romper la localidad de los datos, tal y como otras propuestas vistas han detectado.\\

De este modo, el uso de los PE podría verse gravemente afectado. Desde una perspectiva crítica, el mecanismo \textit{ProbSparse} podría resultar perjudicial para la preservación de la semántica de los datos. Como alternativa, se ha optado por emplear la definición original del Transformer que, aunque presenta una complejidad cuadrática $O(n^2)$, permite evaluar la efectividad real de nuestras propuestas sin comprometer la coherencia semántica.\\

Con esta configuración, basada en el comportamiento tradicional de los Transformers, se dispone de un entorno de comparación justo, libre de operaciones adicionales que puedan distorsionar los resultados o dificultar su análisis, haciendo posible estudiar variables como el impacto del \textit{encoding} frente a la secuencia, y establecer comparaciones con el modelo original de codificación posicional de la forma más directa posible.

\section{Particionado de los datos}

Este aspecto es fundamental en la configuración del entorno de experimentación, ya que el conjunto de validación se emplea para entrenar y ajustar correctamente los modelos, mientras que el de test permite comparar de forma fiable el rendimiento de las propuestas. En ambos casos, debemos cumplir las condiciones expuestas en el Capítulo \ref{particiones}, evitando dependencias entre particiones o la alteración del orden secuencial.\\

Para la partición de test, optaremos por una vía simple, como es la separación de tipo \textit{last-block}, asegurándonos de no tomar algunas de las posiciones entorno al punto de separación para no tener puntos altamente correlados entre las partes. En los datasets escogidos para el análisis comparativo, tomaremos una proporción de particiones de 70-30, que será más que suficiente para obtener resultados fiables en todos los casos.\\
 
También debemos especificar la técnica usada para la partición de validación. Anteriormente (ver \ref{particiones}), examinamos las diferentes alternativas frecuentemente utilizadas para determinar este subconjunto, entre las que destacaba la técnica de validación cruzada por bloques (originalmente descrita en \cite{bergmeir2012use} como \textit{Blocked Cross-validation}). Pero esta alternativa ha de cumplir algunas restricciones, como el principio de estacionariedad, que no siempre se cumple en el estudio de fenómenos reales. Esta limitación, unida a la necesidad de reentrenar el modelo tantas veces como bloques de validación se definan, implica un elevado coste computacional, agravado por el gran número de parámetros del modelo y la considerable longitud de los datos.\\

Por este motivo, en los experimentos que se presentan a continuación se empleará nuevamente el criterio \textit{last-block} para la obtención del conjunto de validación, simplificando así el proceso global de particionado de datos a la técnica más sencilla posible.

\section{Función de pérdida y métricas escogidas}

Tras haber visitado anteriormente en el Capítulo \ref{metricassd} las posibles métricas que podemos emplear, disponemos de un amplio abanico de posibilidades a usar para la comparativa. Para tener una base bastante completa, calcularemos todas ellas en cada entrenamiento, pero nos fijaremos especialmente en dos: MSE y MAE.\\

Como función de pérdida escogeremos MSE, debido a su sencilla interpretación y su habitual uso en problemas de series temporales. De esta forma, podremos comparar los resultados con modelos del estado del arte como Informer. Además, MAE nos ofrece resultados estables y claros cuando MSE no es lo suficientemente discriminatorio, por lo que nos será de gran utilidad para evaluar la calidad de los encodings en datasets con grandes oscilaciones como TINA.\\

La razón de no tener en cuenta para la comparativa directa las métricas MAPE, MSPE y RMSE se debe a su inestabilidad cuando disponemos de valores cercanos a cero. En dichos casos, vimos que se podría producir una explosión descontrolada de los valores de error, por lo que convierte su uso en una alternativa en poco fiable para nuestros conjuntos de datos, los cuales están normalizados y posiblemente tendrán una proporción de valores considerablemente cercana a cero.

\section{Framework de trabajo y especificaciones hardware}

La realización de todo el proceso experimental se realizará haciendo uso de Python~\footnote{Python: \url{https://www.python.org/}}, debido a la gran acogida del lenguaje en el ámbito de la inteligencia artificial, llegando incluso a desplazar a R como principal lenguaje empleado en las publicaciones académicas recientes. Para el desarrollo del trabajo, emplearemos multitud de librerías que facilitan el procesado de datos, pero de entre todas ellas destaca Pytorch~\cite{paszke2019pytorchimperativestylehighperformance}.\\

Su elección viene motivada por su tendencia creciente en las publicaciones científicas, así como su interfaz simplificada frente a TensorFlow. Aunque ambas soluciones nos otorgan funcionalidades equivalentes, las cuales son cada vez más accesibles gracias a diferentes API externas de alto nivel, Pytorch ofrecía un mejor equilibrio entre la baja granularidad de implementación, y el uso de bloques funcionales con los que construir y editar modelos sin entrar en detalles de bajo nivel, como la implementación de las métricas, las capas totalmente conectadas o las convoluciones. Además, el modelo empleado como base, Informer, dispone de una implementación funcional en GitHub~\cite{zhouhaoyi2020informer} realizada en este entorno.\\

En cuanto al preprocesado de los datos, se han empleado las siguientes librerías: NumPy~\cite{harris2020array} para la vectorización de operaciones; Pandas~\cite{mckinney-proc-scipy-2010} para la lectura y transformación de los ficheros \textit{.csv} y \textit{.parquet}; y Matplotlib~\cite{Hunter:2007} y statmodels~\cite{seabold2010statsmodels} para representaciones gráficas y análisis previo de los datos.\\

La ejecución de todos los experimentos se ha realizado haciendo uso del clúster DGX, propiedad de la Universidad de Granada, debido a la necesidad de recursos de cómputo abundantes para la ejecución de cada variante. Los recursos empleados para cada ejecución han sido:

\begin{itemize}
	\item Procesador: x 32 cores Intel® Xeon® E5-2698.
	\item RAM: 96GB
	\item GPU: x 1 Tesla V100 32GB 
\end{itemize}

La ejecución de los experimentos se realiza mediante el script \texttt{run\_exp.py}, disponible en el repositorio del TFM\footnote{Repositorio: \url{https://github.com/hexecoded/tfm}}. Este script permite configurar de manera flexible el modelo mediante numerosos parámetros de entrada, incluyendo la selección de los distintos tipos de codificación posicional y sus hiperparámetros asociados.

