# Página web (Wikipedia)
@misc{bommasani2022opportunitiesrisksfoundationmodels,
	title={On the Opportunities and Risks of Foundation Models}, 
	author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
	year={2022},
	eprint={2108.07258},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2108.07258}, 
}

@misc{zhou2021informerefficienttransformerlong,
	title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting}, 
	author={Haoyi Zhou and Shanghang Zhang and Jieqi Peng and Shuai Zhang and Jianxin Li and Hui Xiong and Wancai Zhang},
	year={2021},
	eprint={2012.07436},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2012.07436}, 
}

@misc{wu2022autoformerdecompositiontransformersautocorrelation,
	title={Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting}, 
	author={Haixu Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},
	year={2022},
	eprint={2106.13008},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2106.13008}, 
}

@misc{zhou2021etdataset,
	author       = {Haoyi Zhou},
	title        = {ETDataset: A Benchmark Dataset for Time-series Forecasting},
	howpublished = {GitHub},
	year         = {2021},
	url          = {https://github.com/zhouhaoyi/ETDataset},
	note         = {Revisado el 12 de agosto de 2025}
}

@misc{irani2025positionalencodingtransformerbasedtime,
	title={Positional Encoding in Transformer-Based Time Series Models: A Survey}, 
	author={Habib Irani and Vangelis Metsis},
	year={2025},
	eprint={2502.12370},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2502.12370}, 
}

@inproceedings{NEURIPS2019_6775a063,
	author = {Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf},
	volume = {32},
	year = {2019}
}

@misc{dai2019transformerxlattentivelanguagemodels,
	title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, 
	author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
	year={2019},
	eprint={1901.02860},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1901.02860}, 
}

@misc{zhou2022fedformerfrequencyenhanceddecomposed,
	title={FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting}, 
	author={Tian Zhou and Ziqing Ma and Qingsong Wen and Xue Wang and Liang Sun and Rong Jin},
	year={2022},
	eprint={2201.12740},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2201.12740}, 
}

@misc{nie2023timeseriesworth64,
	title={A Time Series is Worth 64 Words: Long-term Forecasting with Transformers}, 
	author={Yuqi Nie and Nam H. Nguyen and Phanwadee Sinthong and Jayant Kalagnanam},
	year={2023},
	eprint={2211.14730},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2211.14730}, 
}

@misc{kitaev2020reformerefficienttransformer,
	title={Reformer: The Efficient Transformer}, 
	author={Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
	year={2020},
	eprint={2001.04451},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2001.04451}, 
}

@misc{vaswani2023attentionneed,
	title={Attention Is All You Need}, 
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year={2023},
	eprint={1706.03762},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1706.03762}, 
}

@misc{hebrail2006individual,
	author = {Hebrail, Georges and Berard, Alice},
	title = {Individual Household Electric Power Consumption},
	year = {2010},
	publisher = {UCI Machine Learning Repository},
	doi = {10.24432/C58K54},
	url = {https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption}
}

@misc{nycopendata,
	author       = {{City of New York}},
	title        = {NYC Open Data Portal},
	howpublished = {\url{https://opendata.cityofnewyork.us/data/}},
	note         = {Revisado el 12 de Agosto de 2025}
}


@book{10.5555/574978,
	author = {Box, George Edward Pelham and Jenkins, Gwilym},
	title = {Time Series Analysis, Forecasting and Control},
	year = {1990},
	isbn = {0816211043},
	publisher = {Holden-Day, Inc.},
	address = {USA}
}

@article{10.1162/neco.1997.9.8.1735,
	author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
	title = {Long Short-Term Memory},
	year = {1997},
	issue_date = {November 15, 1997},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	volume = {9},
	number = {8},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	journal = {Neural Comput.},
	month = nov,
	pages = {1735–1780},
	numpages = {46}
}

@misc{tina_dasci_arcelor,
	title={Time-series Industrial Anomaly dataset},
	author={Ignacio Aguilera-Martos and David López and Marta García-Barzana and Julián Luengo and Francisco Herrera},
	year={2022},
	URL={https://github.com/ari-dasci/OD-TINA}
}

@misc{zhouhaoyi2020informer,
	author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
	title = {{Informer2020}: The GitHub repository for Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting-},
	howpublished = {\url{https://github.com/zhouhaoyi/Informer2020}},
	year = {2020—2023},
	note = {Revisado el 12 de agosto de 2025},
}


@InProceedings{ mckinney-proc-scipy-2010,
	author    = { {W}es {M}c{K}inney },
	title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
	booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
	pages     = { 56 - 61 },
	year      = { 2010 },
	editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
	doi       = { 10.25080/Majora-92bf1922-00a }
}

@Article{         harris2020array,
	title         = {Array programming with {NumPy}},
	author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
	van der Walt and Ralf Gommers and Pauli Virtanen and David
	Cournapeau and Eric Wieser and Julian Taylor and Sebastian
	Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
	and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
	Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
	R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
	G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
	Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
	Travis E. Oliphant},
	year          = {2020},
	month         = sep,
	journal       = {Nature},
	volume        = {585},
	number        = {7825},
	pages         = {357--362},
	doi           = {10.1038/s41586-020-2649-2},
	publisher     = {Springer Science and Business Media {LLC}},
	url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@Article{Hunter:2007,
	Author    = {Hunter, J. D.},
	Title     = {Matplotlib: A 2D graphics environment},
	Journal   = {Computing in Science \& Engineering},
	Volume    = {9},
	Number    = {3},
	Pages     = {90--95},
	abstract  = {Matplotlib is a 2D graphics package used for Python for
	application development, interactive scripting, and publication-quality
	image generation across user interfaces and operating systems.},
	publisher = {IEEE COMPUTER SOC},
	doi       = {10.1109/MCSE.2007.55},
	year      = 2007
}

@misc{wu2023timesnettemporal2dvariationmodeling,
	title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis}, 
	author={Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},
	year={2023},
	eprint={2210.02186},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2210.02186}, 
}

@inproceedings{seabold2010statsmodels,
	title={statsmodels: Econometric and statistical modeling with python},
	author={Seabold, Skipper and Perktold, Josef},
	booktitle={9th Python in Science Conference},
	year={2010},
}

@misc{paszke2019pytorchimperativestylehighperformance,
	title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
	author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
	year={2019},
	eprint={1912.01703},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1912.01703}, 
}

@Article{info15090517,
	AUTHOR = {Mienye, Ibomoiye Domor and Swart, Theo G. and Obaido, George},
	TITLE = {Recurrent Neural Networks: A Comprehensive Review of Architectures, Variants, and Applications},
	JOURNAL = {Information},
	VOLUME = {15},
	YEAR = {2024},
	NUMBER = {9},
	ARTICLE-NUMBER = {517},
	URL = {https://www.mdpi.com/2078-2489/15/9/517},
	ISSN = {2078-2489},
	DOI = {10.3390/info15090517}
}

@article{bergmeir2012use,
	title={On the use of cross-validation for time series predictor evaluation},
	author={Bergmeir, Christoph and Benítez, José M},
	journal={Information Sciences},
	volume={191},
	pages={192--213},
	year={2012},
	publisher={Elsevier}
}


@misc{hyndman2021stationarity,
	author    = {Hyndman, Rob J. and Athanasopoulos, George},
	title     = {Forecasting: Principles and Practice - Stationarity and Differencing},
	year      = {2021},
	url       = {https://otexts.com/fpp2/stationarity.html},
	note      = {Revisado el 31 de julio de 2025},
	publisher = {OTexts}
}


@article{taylor2018prophet,
	title     = {Forecasting at scale},
	author    = {Taylor, Sean J. and Letham, Benjamin},
	journal   = {The American Statistician},
	volume    = {72},
	number    = {1},
	pages     = {37--45},
	year      = {2018},
	publisher = {Taylor \& Francis}
}


@book{box1970time,
	title     = {Time Series Analysis: Forecasting and Control},
	author    = {Box, George E. P. and Jenkins, Gwilym M.},
	year      = {1970},
	publisher = {Holden-Day},
	address   = {San Francisco}
}

@misc{prophet2017,
	author    = {Sean J. Taylor and Benjamin Letham},
	title     = {Prophet: Forecasting at Scale},
	year      = {2017},
	url       = {https://research.facebook.com/blog/2017/02/prophet-forecasting-at-scale/},
	urldate   = {2025-07-29},
	note      = {Facebook Research Blog. Revisado el 30/07/2025}
}

@ARTICLE{6795963,
	author={Hochreiter, Sepp and Schmidhuber, Jürgen},
	journal={Neural Computation}, 
	title={Long Short-Term Memory}, 
	year={1997},
	volume={9},
	number={8},
	pages={1735-1780},
	keywords={},
	doi={10.1162/neco.1997.9.8.1735}}
	
@book{box1976time,
		title     = {Time Series Analysis: Forecasting and Control},
		author    = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C.},
		year      = {1976},
		publisher = {Holden-Day},
		note      = {AirPassengers dataset}
	}

@article{cleveland1990stl,
	title     = {STL: A Seasonal-Trend Decomposition Procedure Based on Loess},
	author    = {Cleveland, Robert B. and Cleveland, William S. and McRae, Jean E. and Terpenning, Irma},
	journal   = {Journal of Official Statistics},
	volume    = {6},
	number    = {1},
	pages     = {3--73},
	year      = {1990},
	publisher = {Statistics Sweden}
}

@misc{kingma2017adammethodstochasticoptimization,
	title={Adam: A Method for Stochastic Optimization}, 
	author={Diederik P. Kingma and Jimmy Ba},
	year={2017},
	eprint={1412.6980},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1412.6980}, 
}

@misc{techcharts2012housing,
		author       = {Aksel Kibar},
		title        = {U.S. Housing Starts. U.S. Building Permits},
		year         = {2012},
		howpublished = {\url{https://blog.techcharts.net/index.php/2012/06/19/u-s-housing-starts-u-s-building-permits/}},
		note         = {Revisado el 25 de julio de 2025}
	}